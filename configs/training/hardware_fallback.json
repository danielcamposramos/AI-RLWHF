{
  "cpu": {
    "quantization": "bitsandbytes_4bit",
    "per_device_batch_size": 1,
    "gradient_accumulation_steps": 16,
    "torch_dtype": "bfloat16",
    "use_flash_attention": false
  },
  "mps": {
    "quantization": null,
    "per_device_batch_size": 2,
    "gradient_accumulation_steps": 8,
    "torch_dtype": "float16",
    "use_flash_attention": false
  },
  "ascend_npu": {
    "quantization": "atol",
    "per_device_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "torch_dtype": "float16",
    "use_flash_attention": true
  }
}